---
title: "Description of system performance: using the pipeline v1 as example"
author: "AC"
output: 
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(scales)
```

## A first attempt

The goal here is to link system performance to characteristics of files and/or speakers on files. See
[this file](https://docs.google.com/document/d/1Ef_lr6QAWSa8RKOvC6bbnb6ewd538bBHmQiMYjDRezE/edit) for explanation of the fields.

### Read in descriptor data

We read in background data (only exists for babytrain), data describing files and speakers. Typically, this does not depend on your system results, so you do not need to change it.

```{r readin-descriptors,echo=F, warning=FALSE, comment=FALSE}
# options(warn=2) for debugging, then options(warn=1)

#background information (only available for babytrain)
read.csv("../BabyTrain_ages.csv")->ages
ages[ages$corpus!="corpus",]->ages
ages$age=as.numeric(as.character(ages$age))

allres=dir("../computation/results/",pattern=".csv")
allres=allres[grep("aver",allres,invert=T)]
#descriptors per speaker
datsp=NULL
for(j in allres[grep("perSpeaker",allres)]) { datsp=rbind(datsp,cbind(j,read.csv(paste0("../computation/results/",j))))
datsp$speaker=as.character(datsp$speaker)}
datsp$uniq=paste(datsp$file,datsp$speaker)

#descriptors per file
datf=NULL
for(j in allres[grep("perSpeaker",allres,invert=T)])  datf=rbind(datf,cbind(j,read.csv(paste0("../computation/results/",j))))
datf$cor=gsub("_.*","",datf$j)

datsp=merge(datsp,datf[,c("file","cor")])

#descriptors per speaker
merge(datsp,ages,by.x="file",by.y="basename",all.x=T)->datsp

write.table(aggregate(datf[,4:13],by=list(datf$cor),mean),"../computation/results/_averages.csv",row.names = F)

write.table(aggregate(datsp[,5:7],by=list(datsp$cor),mean),"../computation/results/_averagesPerSpeaker.csv",row.names = F)

merge(datf,ages,by.x="file",by.y="basename",all.x=T)->datf

#show dimensions and summary of the 2 datasets
# dim(datsp)
# summary(datsp)
# table(datsp$cor)
# dim(datf)
# summary(datf)
# table(datf$cor)

write.table(datsp,"temp/datsp.txt",row.names = F)
write.table(datf,"temp/datf.txt",row.names = F)

```

### Read in system evaluation data 

**HUMAN LOOK HERE**
Typically you WILL need to change line 50 below, so that you read in your own system results. Please use pyannote.metrics to generate your results. They should be space separated.

IMPORTANT!!! THERE SHOULD BE NO WARNINGS IN IT. IF THERE ARE WARNINGS, DEAL WITH THEM AND REGENERATE A RESULTS FILE WITHOUT THEM

For this example, I manually removed the warnings in these 4 files...

```{r readin-eval,echo=F, warning=FALSE, comment=FALSE}
myevals=c("/Users/acristia/Documents/gitrepos/corstatana/system_eval/output_rttms/pipeline/v1/lda120_plda_voxceleb_babytrain/jsalt19_spkdiar_babytrain_eval/plda_scores_tbest/result.pyannote-der",
          "/Users/acristia/Documents/gitrepos/corstatana/system_eval/output_rttms/pipeline/v1/lda120_plda_voxceleb_ami/jsalt19_spkdiar_ami_eval_Mix-Headset/plda_scores_tbest/result.pyannote-der",
          "/Users/acristia/Documents/gitrepos/corstatana/system_eval/output_rttms/pipeline/v1/lda120_plda_voxceleb_chime5/jsalt19_spkdiar_chime5_eval_U01/plda_scores_tbest/result.pyannote-der",          "/Users/acristia/Documents/gitrepos/corstatana/system_eval/output_rttms/pipeline/v1/lda120_plda_voxceleb/jsalt19_spkdiar_sri_eval/plda_scores_tbest/result.pyannote-der")
file_eval=NULL
for(thisf in myevals) {
  suppressMessages(suppressWarnings(file_eval <- rbind(file_eval,read_table(thisf, comment = "--"))))
  #print(dim(file_eval))
  }


#summary(file_eval)

#the first col must be renamed so that it has the same name as the description files read above
colnames(file_eval)[1]<-"file"

file_eval=subset(file_eval,file!="TOTAL")

#you may also want to rename the % variables into something that is more readable
colnames(file_eval)[colnames(file_eval)=="%"]<-"cor.pc"
colnames(file_eval)[colnames(file_eval)=="%_1"]<-"fa.pc"
colnames(file_eval)[colnames(file_eval)=="%_2"]<-"miss.pc"
colnames(file_eval)[colnames(file_eval)=="%_3"]<-"conf.pc"
write.table(file_eval,"temp/eval.txt",row.names = F)
```

To make full use of this description suite, you also need to generate a set of descriptors that cross the gold and the system output. To do so, navigate to computation/scripts/ and open the README. Follow the instructions there to set up the analysis environment. Next, assume you have a rttm file with all of the output from a single system for all the wav files. 

```{bash gendesc, eval=F,echo=F}

#
source activate corstatana

# declare which rttm you want to analyze, and where you want to store the results
outfolder="../../system_eval/lda120_plda_voxceleb_samecor_eval_plda_scores_tbest"
myrttm="/Users/acristia/Documents/gitrepos/corstatana/system_eval/output_rttms/pipeline/v1/lda120_plda_voxceleb_babytrain/jsalt19_spkdiar_babytrain_eval/plda_scores_tbest/rttm"

#run the ana
python metrics_by_speaker.py $myrttm BabyTrain.SpeakerDiarization.All test

#REPEAT FOR AMI
myrttm="/Users/acristia/Documents/gitrepos/corstatana/system_eval/output_rttms/pipeline/v1/lda120_plda_voxceleb_ami/jsalt19_spkdiar_ami_eval_Mix-Headset/plda_scores_tbest/rttm"
python metrics_by_speaker.py $myrttm AMI.SpeakerDiarization.Mix-Headset test

#CHIME5
 myrttm="/Users/acristia/Documents/gitrepos/corstatana/system_eval/output_rttms/pipeline/v1/lda120_plda_voxceleb_chime5/jsalt19_spkdiar_chime5_eval_U01/plda_scores_tbest/rttm"
 python metrics_by_speaker.py $myrttm CHiME5.SpeakerDiarization.U01 test

 #AND SRI
  myrttm="/Users/acristia/Documents/gitrepos/corstatana/system_eval/output_rttms/pipeline/v1/lda120_plda_voxceleb/jsalt19_spkdet_sri_eval_test/plda_scores_tbest/rttm"
 python metrics_by_speaker.py $myrttm SRI.SpeakerDiarization.dc test

 
#clean up
mkdir $outfolder

mv *.txt $outfolder/.

#create a single clean file
header="file  ID Ref System  Duration"
echo $header > $outfolder/perSpkall.txt
grep -v "Duration" $outfolder/*perSpk.txt | sed "s~$outfolder~~" | tr ":" "\t" | tr "|" "\t" >> $outfolder/perSpkall.txt

```

This next chunk takes the output of the previous one and combines them into a table that can be used in further analyses

```{r combinesp,echo=F, warning=FALSE, comment=FALSE}
outfolder="../system_eval/lda120_plda_voxceleb_samecor_eval_plda_scores_tbest/"

read.table(paste0(outfolder,"perSpkall.txt"),header=T)->spout
#summary(spout)
#dim(spout)
spout$file=gsub("_perSpk.txt","",gsub("/","",spout$file,fixed=T))
spout$uniq=paste(spout$file,spout$ID)

# convert to rates
# false_disc = ref other, system speak
# missed_disc = ref speak, system other speak
# missed_speech = ref speak, system no_speaker
# correct = ref speak, system speak
# total speaker = ref speak tot
# total others= ref other

spr=NULL
for(u in levels(factor(spout$uniq))){
  # false_disc = ref other, system speak
  fd=spout[spout$uniq==u & spout$Ref=="other-speaker" & spout$System=="speaker","Duration"]
  
# missed_disc = ref speak, system other speak
  md=spout[spout$uniq==u & spout$Ref=="speaker" & spout$System=="other-speaker","Duration"]

  # missed_speech = ref speak, system no_speaker
  ms=spout[spout$uniq==u & spout$Ref=="speaker" & spout$System=="no-speaker","Duration"]

# correct = ref speak, system speak
  c=spout[spout$uniq==u & spout$Ref=="speaker" & spout$System=="speaker","Duration"]

# total speaker = ref speak tot
  ts=sum(spout[spout$uniq==u & spout$Ref=="speaker" ,"Duration"],na.rm=T)

# total others= ref other
   to=sum(spout[spout$uniq==u & spout$Ref=="other-speaker" ,"Duration"],na.rm=T)

spr=rbind(spr,cbind(u,fd,md,ms,c,ts,to))
}
#summary(spr)

spr=merge(spr,datsp,by.x="u",by.y="uniq")
write.table(spr,paste0(outfolder,"speaker_ana.txt"),row.names=F)
write.table(spr,"temp/speaker_ana.txt",row.names = F)

```

**HUMAN END OF LOOK HERE**

### Combine descriptor and system data 

If all goes well, you won't need to change any of the following sections. After this code, the table file_eval has a combination of results and descriptors at the level of files.

```{r mrg,echo=F, warning=FALSE, comment=FALSE}

merge(file_eval,datf,all.x=T)->file_eval

#dim(file_eval) #**human** check that the number of rows (first number) outputted here is the same as that in line 61.
#summary(file_eval)
write.table(file_eval,"temp/evalwdesc.txt",row.names = F)

```

Now you are ready to do some inspection. You can turn chunks off by adding ", eval=F" (e.g. {r spl,fig.height=10} below, it would become {r spl,fig.height=10, eval=F}) 

### Example of analysis: Focusing on our pipeline V1

A scatter plot matrix shows many bivariate plots. In the one below, we focus exclusively on descriptors at the level of the file.

```{r spl,fig.height=10,echo=F}
file_eval$total.capped=file_eval$total
file_eval$total.capped[file_eval$total.capped>2000]=2000
file_eval$fa.pc.capped=file_eval$fa.pc
file_eval$fa.pc.capped[file_eval$fa.pc.capped>200]=200
#pdf("pipelineres.pdf",height=10,width=10)
library(lattice)
selected=c("fa.pc.capped","miss.pc","conf.pc","total.capped","prop_ovl_speech","nb_diff_speakers")
selnames=gsub(".","\n",gsub("_","\n",selected),fixed=T)
splom(file_eval[c(selected)],groups=file_eval$cor,varnames=selnames,auto.key = list(columns = 3),axis.line.tck = -.5, axis.text.lineheight=0,
panel = function(x, y, ...) {
           panel.xyplot(x, y, ...)
          fm <- lm(y ~ x)
          panel.abline(fm,col.line = "gray")
}
)
#dev.off()
```

Focus on the last three rows, which show the correlations between percent false alarms (last row -- capped at 200, meaning that any file that had more than 200 gets 200), percent misses (penultimate row), and percent confusion (antepen row),  and the following selected characteristics (from left to right):

- proportion of speech that is overlapping (out of all speech in the file, which part was overlapping between two talkers)
- number of different speakers

For example, focusing on the last row, there is a lower percent of false alarms for corpora with more overlap (although this is mainly driven by corpus differences).

One row up, percent misses goes up with proportion of overlap (same caveat).

One row up, percent confusion goes down slightly with proportion overlap (but same caveat); and it goes up with number of talkers.

<!-- Now we do the same to see which speakers are picked up correctly or not. NOTE -- THERE IS A BUG UPSTREAM SO THIS IS SOLVED ONLY AT THE VAD LEVEL, NOT AT THE TD LEVEL... (all cases of error due to confusion are na, which is false, obviously) - also why is there only babytrain? -->

### Example of analysis 2: Still using the pipeline v1

You can also focus on specific outcome and predictor variables and trim their distribution to see them more clearly.

```{r indiv-plots,echo=F}

cor_color=rainbow(length(levels(factor(file_eval$cor)))) #get different colors for diff datasets
names(cor_color)<-levels(factor(file_eval$cor))

file_eval_metrics=c("fa.pc.capped","miss.pc","conf.pc")
predictors=c("prop_ovl_speech","nb_diff_speakers")

for(thismet in file_eval_metrics){ 
  iqr=IQR(file_eval[,thismet])
  med=median(file_eval[,thismet])
  no_outliers=file_eval[file_eval[,thismet]<med+1.5*iqr,]
  print(paste("removing",dim(file_eval)[1]-dim(no_outliers)[1], "outliers in",thismet,"specifically the following files:"))
  print(file_eval[file_eval[,thismet]>=med+1.5*iqr,"file"])
  for(thispred in predictors){
    plot(no_outliers[,thismet]~no_outliers[,thispred], pch=20,col=alpha(cor_color[no_outliers$corpus],.2),xlab=thispred,ylab=thismet)
    abline(lm(no_outliers[,thismet]~no_outliers[,thispred]))
    print(summary(lm(no_outliers[,thismet]~no_outliers[,thispred])))
    if(max(no_outliers[,thismet])>300){
        plot(no_outliers[,thismet]~no_outliers[,thispred], pch=20,col=alpha(cor_color[no_outliers$corpus],.2),xlab=thispred,ylab=paste(thismet,"(restricted range)"),ylim=c(0,300))
    abline(lm(no_outliers[,thismet]~no_outliers[,thispred]))      
    }
  }
}


```

Messages I take away from this:

*For FA rate capped*

(conclusions to be taken with a grain of salt, given that capping shifts the distribution)

-  significantly lower FA  with higher proportion of overlapping speech
- no sig different FA as a function of number of different speakers


*For miss rate*

Attention that most SRI files get excluded as outliers for miss rate...

- sig higher miss for files with higher prop overlapping speech
- sig higher miss when more speakers

*For confusion*


- sig lower confusion for files with higher prop overlapping speech
- sig higher confusion when more speakers