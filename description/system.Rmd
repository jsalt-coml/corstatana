---
title: "Description of system performance"
author: "AC"
output: 
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

## A first attempt

The goal here is to link system performance to characteristics of files and/or speakers on files. See
[this file](https://docs.google.com/document/d/1Ef_lr6QAWSa8RKOvC6bbnb6ewd538bBHmQiMYjDRezE/edit) for explanation of the fields.

### Read in descriptor data

We read in background data (only exists for babytrain), data describing files and speakers. Typically, this does not depend on your system results, so you do not need to change it.

```{r readin-descriptors}
# options(warn=2) for debugging, then options(warn=1)

read.csv("../BabyTrain_ages.csv")->ages
ages[ages$corpus!="corpus",]->ages
ages$age=as.numeric(as.character(ages$age))

allres=dir("../computation/results/",pattern=".csv")

#descriptors per speaker
datsp=NULL
for(j in allres[grep("perSpeaker",allres)])  datsp=rbind(datsp,cbind(j,read.csv(paste0("../computation/results/",j))))

#descriptors per file
datf=NULL
for(j in allres[grep("perSpeaker",allres,invert=T)])  datf=rbind(datf,cbind(j,read.csv(paste0("../computation/results/",j))))

merge(datsp,ages,by.x="file",by.y="basename",all.x=T)->datsp
merge(datf,ages,by.x="file",by.y="basename",all.x=T)->datf

dim(datsp)
summary(datsp)
dim(datf)
summary(datf)
```

### Read in system data **HUMAN LOOK HERE**

Typically you WILL need to change line 50 below, so that you read in your own system results. Please use pyannote.metrics to generate your results. They should be space separated 

```{r readin-eval}
library(readr)
out <- read_table("../system_eval/BabyTrain_ConvRNN.txt", comment = "--")

dim(out)
summary(out)
merge(out,datf,all=T)->out

summary(out)
```


The table out has a combination of results and descriptors at the level of files.

### Explaining misses and false alarms across files based on file characteristics (Marvin VAD on BabyTrain - 5 class, old architecture)

A scatter plot matrix shows many bivariate plots. In the one below, we focus exclusively on descriptors at the level of the file and only for BabyTrain because that's what I drew results for. We only have false alarms and misses because we are looking at a VAD system. (In particular, this is Marvin's system for last week.)

```{r spl,fig.height=10}

library(lattice)
selected=c("fa.pc","miss.pc","prop_ovl_speech","avg_voc_dur","age","snr","nb_diff_speakers")
selnames=gsub(".","\n",gsub("_","\n",selected),fixed=T)
splom(out[c(selected)],pch=".",groups=out$corpus,varnames=selnames,auto.key = list(columns = 3),axis.line.tck = -.5, axis.text.lineheight=0,
panel = function(x, y, ...) {
           panel.xyplot(x, y, ...)
          fm <- lm(y ~ x)
          panel.abline(fm,col.line = "gray")
}
)
```

Focus on the last two rows, which show the correlations between percent misses (penultimate row) or percent false alarms (last row) and the following selected characteristics (from left to right):

- proportion of speech that is overlapping
- average vocalization/utterance/sentence duration
- key child age
- SNR calculated as RMS(x_speech)/RMS(x_sil) where x_speech is an array with all the areas of speech in the gold annotation, and  x_sil is un array with all the areas of silence
- number of different speakers

So focusing on the last row, false alarms look unrelated to all of these predictors, although this may be because the scale is too large.

One row up, misses does not relate to proportion overlap or number of different speakers, but is anticorrelated with the duration of speech, child age, and SNR.